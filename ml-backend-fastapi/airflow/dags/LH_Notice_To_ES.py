from datetime import datetime, timedelta

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator

from plugins.crawlers.lh_crawler_for_elastic import collect_lh_file_urls_and_pdf # í¬ë¡¤ë§ ëª¨ë“ˆ import
from plugins.crawlers.lh_crawler_for_elastic import CommonConfig

from elasticsearch import Elasticsearch

import json
import shutil
import pendulum
import logging
import os
import time
import psutil

# íƒ€ì„ì¡´ ì„¤ì • (Asia/Seoul ê¸°ì¤€)
local_tz = pendulum.timezone("Asia/Seoul")

# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'airflow',
    'depends_on_past': True,
    'start_date': datetime(2025, 5, 14, tzinfo=local_tz),
    'retries': 3,
    'retry_delay': timedelta(seconds=10), # ì¬ì‹œë„ ê°„ê²©
    'execution_timeout':timedelta(hours=72), # ì‹¤í–‰ ì‹œê°„ ì œí•œ
    'email_on_failure': False
    # 'email': ['admin@example.com']
}

logger = logging.getLogger('airflow.task')

# DAG ì •ì˜
with DAG(
    dag_id='PDF_CRAWLER_TO_ES',
    default_args=default_args,
    schedule="@daily",
    catchup=True,
    max_active_runs=1,  # ë™ì‹œ ì‹¤í–‰ ë°©ì§€
    max_active_tasks=4,  # concurrency ëŒ€ì‹  max_active_tasks ì‚¬ìš©
    tags=['pdf', 'crawler', 'elasticsearch'],
    doc_md="""
    # PDF í¬ë¡¤ëŸ¬ ë° Elasticsearch ì ì¬ DAG
    
    ## ê°œìš”
    LH ê³µê³  PDFë¥¼ í¬ë¡¤ë§í•˜ê³  Elasticsearchì— ì ì¬í•˜ëŠ” DAG
    
    ## íƒœìŠ¤í¬ êµ¬ì„±
    1. **check_system_resources**: ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì‚¬ì „ ì²´í¬
    2. **crawl_pdfs**: PDF íŒŒì¼ í¬ë¡¤ë§
    3. **process_to_es**: PDF ì²˜ë¦¬ ë° ES ì ì¬
    4. **organize_files**: íŒŒì¼ ì •ë¦¬ ë° ë¶„ë¥˜
    5. **check_system_resources_after**: ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì‚¬í›„ ì²´í¬
    
    ## ì£¼ì˜ì‚¬í•­
    - ë””ìŠ¤í¬ ê³µê°„ ìµœì†Œ 5GB í•„ìš”
    - ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  85% ì´ìƒ ì‹œ ì£¼ì˜
    """
) as dag:
    # ==================================================================
    # TASK 0: í¬ë¡¤ë§ ì‹œì‘ ì „ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì²´í¬
    # ==================================================================
    def check_system_resources(**context):
        """í¬ë¡¤ë§ ì‹œì‘ ì „ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì²´í¬"""
        from airflow.exceptions import AirflowException
        
        logger.info("í¬ë¡¤ë§ ì‹œì‘ ì „ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì²´í¬")
        
        # ë¦¬ì†ŒìŠ¤ ì„ê³„ê°’ ì„¤ì •
        RESOURCE_THRESHOLDS = {
            'cpu_percent': 80,
            'memory_percent': 85,
            'disk_percent': 90,
            'min_disk_gb': 5
        }
        
        # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì²´í¬
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')
        network_before = psutil.net_io_counters()
        
        # ë¦¬ì†ŒìŠ¤ ë¶€ì¡± ì‹œ ê²½ê³  ë˜ëŠ” ì‹¤íŒ¨ ì²˜ë¦¬
        warnings = []
        
        if cpu_percent > RESOURCE_THRESHOLDS['cpu_percent']:
            warnings.append(f"CPU ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤: {cpu_percent}%")
        
        if memory.percent > RESOURCE_THRESHOLDS['memory_percent']:
            warnings.append(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤: {memory.percent}%")
        
        if disk.percent > RESOURCE_THRESHOLDS['disk_percent']:
            warnings.append(f"ë””ìŠ¤í¬ ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤: {disk.percent}%")
        
        if disk.free / 1024 / 1024 / 1024 < RESOURCE_THRESHOLDS['min_disk_gb']:
            warnings.append(f"ì‚¬ìš© ê°€ëŠ¥í•œ ë””ìŠ¤í¬ ê³µê°„ì´ ë¶€ì¡±í•©ë‹ˆë‹¤: {disk.free / 1024 / 1024 / 1024:.1f} GB")
        
        if warnings:
            for warning in warnings:
                logger.warning(f"âš ï¸ {warning}")
            
            # ì‹¬ê°í•œ ê²½ìš° íƒœìŠ¤í¬ ì‹¤íŒ¨ ì²˜ë¦¬
            if disk.free / 1024 / 1024 / 1024 < 1:  # 1GB ë¯¸ë§Œ
                raise AirflowException("ë””ìŠ¤í¬ ê³µê°„ì´ ë¶€ì¡±í•˜ì—¬ í¬ë¡¤ë§ì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # í˜„ì¬ ì‹œìŠ¤í…œ ìƒíƒœ ë¡œê¹…
        logger.info(f"ğŸ“Š ì‹œì‘ ì „ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í˜„í™©:")
        logger.info(f"- CPU ì‚¬ìš©ë¥ : {cpu_percent}%")
        logger.info(f"- ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {memory.percent}%")
        logger.info(f"- ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬: {memory.available / 1024 / 1024:.0f} MB")
        logger.info(f"- ë””ìŠ¤í¬ ì‚¬ìš©ë¥ : {disk.percent}%")
        logger.info(f"- ì‚¬ìš© ê°€ëŠ¥ ë””ìŠ¤í¬: {disk.free / 1024 / 1024 / 1024:.1f} GB")

        # ë„¤íŠ¸ì›Œí¬ ê¸°ì¤€ì  ì €ì¥ (í¬ë¡¤ë§ í›„ ë¹„êµìš©)
        baseline_data = {
            'timestamp': time.time(),
            'cpu_percent': cpu_percent,
            'memory_used': memory.used,
            'memory_percent': memory.percent,
            'disk_percent': disk.percent,
            'network_bytes_sent': network_before.bytes_sent,
            'network_bytes_recv': network_before.bytes_recv,
            'network_packets_sent': network_before.packets_sent,
            'network_packets_recv': network_before.packets_recv,
            'warnings': warnings  # ê²½ê³  ì‚¬í•­ë„ ì €ì¥
        }
        
        # XComìœ¼ë¡œ ê¸°ì¤€ ë°ì´í„° ì „ë‹¬
        context['ti'].xcom_push(key='resource_baseline', value=baseline_data)
        logger.info("âœ… ë¦¬ì†ŒìŠ¤ ê¸°ì¤€ì  ì„¤ì • ì™„ë£Œ")

    check_system_resources_task = PythonOperator(
        task_id='check_system_resources',
        python_callable=check_system_resources
    )

    # ==================================================================
    # TASK 1: PDF í¬ë¡¤ë§ (í¬ë¡¤ë§ ëª¨ë“ˆ ì‹¤í–‰)
    # ==================================================================
    def crawl_pdfs(**context):
        """ì™¸ë¶€ í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ë° ì‹¤íŒ¨ ê¸°ë¡ ê´€ë¦¬"""
        
        task_logger = context.get('logger') or logger
        
        # íƒœìŠ¤í¬ ì‹¤í–‰ ì‹œì ì— ë””ë ‰í† ë¦¬ ìƒì„±
        try:
            CommonConfig.ensure_directories()
            task_logger.info("í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„± ì™„ë£Œ")
        except Exception as e:
            task_logger.warning(f"ë””ë ‰í† ë¦¬ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            # ìµœì†Œí•œì˜ ë””ë ‰í† ë¦¬ëŠ” ì§ì ‘ ìƒì„±
            import os
            try:
                os.makedirs("/opt/airflow/downloads/normal_dag", exist_ok=True)
                task_logger.info("ê¸°ë³¸ ë‹¤ìš´ë¡œë“œ ë””ë ‰í† ë¦¬ ìƒì„± ì™„ë£Œ")
            except Exception as dir_error:
                task_logger.error(f"ê¸°ë³¸ ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨: {dir_error}")
                raise
        execution_date = context.get('ds')
        execution_date = datetime.strptime(execution_date, "%Y-%m-%d").date()
        
        try:
            task_logger.info("ğŸ“¥ PDF í¬ë¡¤ë§ ì‹œì‘")
            
            # í¬ë¡¤ë§ ëª¨ë“ˆ ì‹¤í–‰ (ì„±ê³µ ì‹œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ)
            crawl_result = collect_lh_file_urls_and_pdf(
                "https://apply.lh.or.kr",
                "https://apply.lh.or.kr/lhapply/apply/wt/wrtanc/selectWrtancList.do?viewType=srch",
                "https://apply.lh.or.kr/lhapply/lhFile.do",
                "/opt/airflow/downloads/normal_dag",
                {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"},
                execution_date
            ) 
            
            # XComì— í¬ë¡¤ë§ ê¸°ë¡ ì „ë‹¬
            context['ti'].xcom_push(key='crawl_result', value=crawl_result)
            
        except Exception as e:
            task_logger.error(f"âŒ í¬ë¡¤ë§ ëª¨ë“ˆ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
            raise

    crawl_task = PythonOperator(
        task_id='crawl_pdfs',
        python_callable=crawl_pdfs
    )

    # ==================================================================
    # TASK 2: PDF ì²˜ë¦¬ ë° ES ì ì¬ (ì œê³µëœ main.py ì‹¤í–‰)
    # ==================================================================
    def process_to_es(**context):
        """PDF ì²˜ë¦¬ ë° ES ì ì¬ (main.py ì‹¤í–‰)"""
        import subprocess
        from airflow.exceptions import AirflowException
        
        task_logger = context.get('logger') or logger
        
        # ì´ì „ íƒœìŠ¤í¬ì—ì„œ í¬ë¡¤ë§ ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
        crawl_result = context['ti'].xcom_pull(key='crawl_result', task_ids='crawl_pdfs')
        
        if not crawl_result:
            logger.warning("âš ï¸ ìˆ˜ì§‘ëœ ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤ - ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŒ")
            return {'status': 'no_files', 'processed': 0}
        
        # crawl_result ë¦¬ìŠ¤íŠ¸ì—ì„œ pdf íŒŒì¼ëª…ë§Œ ì¶”ì¶œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥
        pdf_files = [item[1] for item in crawl_result]
        logger.info(f"ğŸ“‹ ì´ì „ íƒœìŠ¤í¬ì—ì„œ ìˆ˜ì§‘ëœ ê³µê³  pdf íŒŒì¼ìˆ˜: {len(pdf_files)}ê°œ")
        logger.info(f"ğŸ“‹ ì´ì „ íƒœìŠ¤í¬ì—ì„œ ìˆ˜ì§‘ëœ ê³µê³  pdf íŒŒì¼ëª…: {pdf_files}")

        # JSON í˜•ì‹ìœ¼ë¡œ í™˜ê²½ë³€ìˆ˜ ì „ë‹¬
        os.environ['PROCESS_PDF_FILES'] = json.dumps(pdf_files, ensure_ascii=False)
        logger.info(f"ğŸ“‹ í™˜ê²½ë³€ìˆ˜ë¡œ ì „ë‹¬ëœ íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸ (JSON): {os.environ['PROCESS_PDF_FILES']}")
        
        # subprocess ì‹¤í–‰ ë¶€ë¶„ ê°œì„ 
        try:
            # main.py ì‹¤í–‰ (í™˜ê²½ ë³€ìˆ˜ ì „ë‹¬)
            task_logger.info("ğŸ”„ main.py ì‹¤í–‰ ì‹œì‘")

            process = subprocess.Popen(
                ['python', '/opt/airflow/plugins/processors/main.py'],
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,  # stderrë¥¼ stdoutìœ¼ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
                text=True,
                bufsize=1,  # ë¼ì¸ ë²„í¼ë§
                env=os.environ.copy()  # í™˜ê²½ë³€ìˆ˜ ëª…ì‹œì  ì „ë‹¬
            )
            
            # ì‹¤ì‹œê°„ ë¡œê·¸ ì¶œë ¥
            for line in iter(process.stdout.readline, ''):
                if line:
                    task_logger.info(line.strip())
            
            return_code = process.wait()
            
            if return_code != 0:
                # ì—ëŸ¬ ë©”ì‹œì§€ ê°œì„ 
                error_msg = f"main.py ì‹¤í–‰ ì‹¤íŒ¨ - ë°˜í™˜ ì½”ë“œ: {return_code}"
                task_logger.error(f"âŒ {error_msg}")
                
                # ì‹¤íŒ¨ íŒŒì¼ ëª©ë¡ ì½ê¸°
                execution_date = context.get('ds')
                execution_date = datetime.strptime(execution_date, "%Y-%m-%d").date()
                failed_file_path = f"/opt/airflow/downloads/normal_dag/esupload_failed_records/{execution_date}_failed_files.json"
                
                failed_files = []
                try:
                    if os.path.exists(failed_file_path):
                        with open(failed_file_path, 'r') as f:
                            failed_data = json.load(f)
                            failed_files = failed_data.get('failed_files', [])
                            task_logger.info(f"ğŸ“‹ ì‹¤íŒ¨ íŒŒì¼ ëª©ë¡ ë¡œë“œ: {len(failed_files)}ê°œ")
                    else:
                        task_logger.warning(f"âš ï¸ ì‹¤íŒ¨ íŒŒì¼ ëª©ë¡ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {failed_file_path}")
                        
                except Exception as read_error:
                    task_logger.error(f"âŒ ì‹¤íŒ¨ íŒŒì¼ ëª©ë¡ ì½ê¸° ì‹¤íŒ¨: {str(read_error)}")
                
                # XComì— ì‹¤íŒ¨ íŒŒì¼ ëª©ë¡ ì €ì¥
                context['ti'].xcom_push(key='failed_files', value=failed_files)
                
                raise AirflowException(error_msg)  # Airflow ì „ìš© ì˜ˆì™¸ ì‚¬ìš©
                
            task_logger.info("âœ… main.py ì‹¤í–‰ ì¢…ë£Œ - ì ì¬ ì™„ë£Œ")
            
            # ì„±ê³µ ì‹œì—ë„ ì‹¤íŒ¨ íŒŒì¼ í™•ì¸
            execution_date = context.get('ds')
            execution_date = datetime.strptime(execution_date, "%Y-%m-%d").date()
            failed_file_path = f"/opt/airflow/downloads/normal_dag/esupload_failed_records/{execution_date}_failed_files.json"
            
            failed_files = []
            if os.path.exists(failed_file_path):
                with open(failed_file_path, 'r') as f:
                    failed_data = json.load(f)
                    failed_files = failed_data.get('failed_files', [])
                    
            context['ti'].xcom_push(key='failed_files', value=failed_files)
            
            return {'status': 'success', 'total': len(pdf_files), 'failed': len(failed_files)}
        
        except Exception as e:
            task_logger.error(f"âŒ main.py ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise

    process_task = PythonOperator(
        task_id='process_to_es',
        python_callable=process_to_es
    )

    # ==================================================================
    # TASK 3: íŒŒì¼ ì •ë¦¬
    # ==================================================================
    def organize_files(**context):
        """ì„±ê³µ/ì‹¤íŒ¨ íŒŒì¼ ë¶„ë¥˜ ë° ì˜¤ë˜ëœ íŒŒì¼ ì •ë¦¬"""
        ti = context['ti']
        task_logger = context.get('logger') or logger
        
        # ì„¤ì •
        RETENTION_DAYS = 7  # íŒŒì¼ ë³´ê´€ ê¸°ê°„
        
        # ES ì ì¬ ì‹¤íŒ¨ íŒŒì¼ ëª©ë¡ ì¡°íšŒ
        failed_files = ti.xcom_pull(key='failed_files', task_ids='process_to_es') or []
        task_logger.info(f"ğŸ“‹ ì‹¤íŒ¨ íŒŒì¼ ëª©ë¡: {failed_files}")
        
        # ë””ë ‰í† ë¦¬ ì„¤ì •
        dirs = {
            'source': '/opt/airflow/downloads/normal_dag',
            'success': '/opt/airflow/downloads/normal_dag/es_success_pdf',
            'failed': '/opt/airflow/downloads/normal_dag/es_failed_pdf'
        }
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        for dir_path in dirs.values():
            os.makedirs(dir_path, exist_ok=True)
        
        # í†µê³„ ì´ˆê¸°í™”
        stats = {
            'moved_success': 0,
            'moved_failed': 0,
            'errors': 0,
            'cleaned_old': 0
        }
        
        # 1. íŒŒì¼ ì´ë™
        try:
            # ì‹¤ì œ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ì—ì„œ PDF íŒŒì¼ ì°¾ê¸°
            if os.path.exists(dirs['source']):
                for file_name in os.listdir(dirs['source']):
                    if not file_name.endswith('.pdf'):
                        continue
                        
                    source_path = os.path.join(dirs['source'], file_name)
                    
                    # íŒŒì¼ì¸ì§€ í™•ì¸ (ë””ë ‰í† ë¦¬ ì œì™¸)
                    if not os.path.isfile(source_path):
                        continue
                    
                    try:
                        # ì‹¤íŒ¨ ëª©ë¡ì— ìˆëŠ”ì§€ í™•ì¸
                        if file_name in failed_files:
                            dest_dir = dirs['failed']
                            stats_key = 'moved_failed'
                        else:
                            dest_dir = dirs['success']
                            stats_key = 'moved_success'
                        
                        dest_path = os.path.join(dest_dir, file_name)
                        
                        # ì¤‘ë³µ íŒŒì¼ ì²˜ë¦¬
                        if os.path.exists(dest_path):
                            # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
                            base_name, ext = os.path.splitext(file_name)
                            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                            new_name = f"{base_name}_{timestamp}{ext}"
                            dest_path = os.path.join(dest_dir, new_name)
                        
                        # íŒŒì¼ ì´ë™
                        shutil.move(source_path, dest_path)
                        stats[stats_key] += 1
                        task_logger.info(f"ğŸ“¦ {file_name} â†’ {os.path.basename(dest_dir)}")
                        
                    except Exception as move_error:
                        stats['errors'] += 1
                        task_logger.error(f"âŒ íŒŒì¼ ì´ë™ ì‹¤íŒ¨ {file_name}: {move_error}")
            else:
                task_logger.warning(f"âš ï¸ ì†ŒìŠ¤ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {dirs['source']}")
        
        except Exception as e:
            task_logger.error(f"âŒ íŒŒì¼ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            raise
        
        # 2. ì˜¤ë˜ëœ íŒŒì¼ ì •ë¦¬ (ì„ íƒì )
        if RETENTION_DAYS > 0:
            cutoff_date = datetime.now() - timedelta(days=RETENTION_DAYS)
            
            for dir_name in ['success', 'failed']:
                dir_path = dirs[dir_name]
                try:
                    for file_name in os.listdir(dir_path):
                        file_path = os.path.join(dir_path, file_name)
                        if os.path.isfile(file_path):
                            file_mtime = datetime.fromtimestamp(os.path.getmtime(file_path))
                            if file_mtime < cutoff_date:
                                os.remove(file_path)
                                stats['cleaned_old'] += 1
                                task_logger.info(f"ğŸ—‘ï¸ ì˜¤ë˜ëœ íŒŒì¼ ì‚­ì œ: {file_name}")
                except Exception as e:
                    task_logger.error(f"ì˜¤ë˜ëœ íŒŒì¼ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # ê²°ê³¼ ìš”ì•½
        task_logger.info(f"""
        ğŸ“Š íŒŒì¼ ì •ë¦¬ ì™„ë£Œ:
        - ì„±ê³µ í´ë”ë¡œ ì´ë™: {stats['moved_success']}ê°œ
        - ì‹¤íŒ¨ í´ë”ë¡œ ì´ë™: {stats['moved_failed']}ê°œ
        - ì˜¤ë¥˜ ë°œìƒ: {stats['errors']}ê°œ
        - ì˜¤ë˜ëœ íŒŒì¼ ì‚­ì œ: {stats['cleaned_old']}ê°œ
        """)
        
        # XComìœ¼ë¡œ í†µê³„ ì „ë‹¬
        context['ti'].xcom_push(key='organize_stats', value=stats)

    organize_task = PythonOperator(
        task_id='organize_files',
        python_callable=organize_files
    )

    # ==================================================================
    # TASK 4: í¬ë¡¤ë§ ë° ES ì ì¬ í›„ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì²´í¬
    # ==================================================================
    def check_system_resources_after(**context):
        """í¬ë¡¤ë§ ë° ES ì ì¬ í›„ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì²´í¬"""
        logger.info("í¬ë¡¤ë§ ë° ES ì ì¬ í›„ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì²´í¬")
        
        # ì´ì „ íƒœìŠ¤í¬ì—ì„œ ê¸°ì¤€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        baseline_data = context['ti'].xcom_pull(key='resource_baseline', task_ids='check_system_resources')
        
        if not baseline_data:
            logger.warning("âš ï¸ ê¸°ì¤€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤ - ë¦¬ì†ŒìŠ¤ ë¶„ì„ ë¶ˆê°€")
            return
        
        # í˜„ì¬ ì‹œìŠ¤í…œ ìƒíƒœ ì²´í¬
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')
        network_after = psutil.net_io_counters()
        
        # ë¦¬ì†ŒìŠ¤ ë³€ë™ ê³„ì‚°
        resource_diff = {
            'cpu_percent': cpu_percent - baseline_data['cpu_percent'],
            'memory_used': memory.used - baseline_data['memory_used'],
            'memory_percent': memory.percent - baseline_data['memory_percent'],
            'disk_percent': disk.percent - baseline_data['disk_percent'],
            'network_bytes_sent': network_after.bytes_sent - baseline_data['network_bytes_sent'],
            'network_bytes_recv': network_after.bytes_recv - baseline_data['network_bytes_recv'],
            'network_packets_sent': network_after.packets_sent - baseline_data['network_packets_sent'],
            'network_packets_recv': network_after.packets_recv - baseline_data['network_packets_recv']
        }
        
        # ë¦¬ì†ŒìŠ¤ ë³€ë™ ë¡œê¹…
        logger.info(f"ğŸ“Š ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ë³€ë™ í˜„í™©:")
        logger.info(f"- CPU ì‚¬ìš©ë¥  ë³€ë™: {resource_diff['cpu_percent']}%")
        logger.info(f"- ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ë³€ë™: {resource_diff['memory_percent']}%")
        logger.info(f"- ë””ìŠ¤í¬ ì‚¬ìš©ë¥  ë³€ë™: {resource_diff['disk_percent']}%")
        logger.info(f"- ë„¤íŠ¸ì›Œí¬ ì†¡ì‹  ë³€ë™: {resource_diff['network_bytes_sent']} bytes")
        logger.info(f"- ë„¤íŠ¸ì›Œí¬ ìˆ˜ì‹  ë³€ë™: {resource_diff['network_bytes_recv']} bytes")
        logger.info(f"- ë„¤íŠ¸ì›Œí¬ íŒ¨í‚· ì†¡ì‹  ë³€ë™: {resource_diff['network_packets_sent']} packets")
        logger.info(f"- ë„¤íŠ¸ì›Œí¬ íŒ¨í‚· ìˆ˜ì‹  ë³€ë™: {resource_diff['network_packets_recv']} packets")

        # ë¦¬ì†ŒìŠ¤ ë³€ë™ í‰ê°€
        if resource_diff['cpu_percent'] > 10:
            logger.warning(f"âš ï¸ CPU ì‚¬ìš©ë¥ ì´ 10% ì´ìƒ ì¦ê°€í–ˆìŠµë‹ˆë‹¤ ({resource_diff['cpu_percent']}%)")
        if resource_diff['memory_percent'] > 10:
            logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ 10% ì´ìƒ ì¦ê°€í–ˆìŠµë‹ˆë‹¤ ({resource_diff['memory_percent']}%)")
        
        # XComìœ¼ë¡œ ë¦¬ì†ŒìŠ¤ ë³€ë™ ë°ì´í„° ì „ë‹¬
        context['ti'].xcom_push(key='resource_diff', value=resource_diff)
        logger.info("âœ… ë¦¬ì†ŒìŠ¤ ë³€ë™ ë°ì´í„° ì „ë‹¬ ì™„ë£Œ")

    check_system_resources_after_task = PythonOperator(
        task_id='check_system_resources_after',
        python_callable=check_system_resources_after
    )

    # ==================================================================
    # TASK ì˜ì¡´ì„± ì„¤ì •
    # ==================================================================
    check_system_resources_task >> crawl_task >> process_task >> organize_task >> check_system_resources_after_task
