# -*- coding: utf-8 -*-
from datetime import datetime, timedelta
import os
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable
from airflow.hooks.base import BaseHook

# ëª¨ë“ˆ ìž„í¬íŠ¸
from plugins.utils.file_helpers import ensure_directory
from plugins.crawlers.lh_crawler import collect_lh_file_urls
from plugins.processors.pdf_processors import PDFProcessor
from plugins.processors.es_uploaders import process_single_pdf
from plugins.processors.es_uploaders import get_elasticsearch_client

# í™˜ê²½ ì„¤ì • - ë³€ìˆ˜ ì •ì˜
BASE_URL = "https://apply.lh.or.kr"
LIST_URL = f"{BASE_URL}/lhapply/apply/wt/wrtanc/selectWrtancList.do?viewType=srch"
DOWNLOAD_URL = f"{BASE_URL}/lhapply/lhFile.do"
DOWNLOAD_DIR = "/opt/airflow/downloads"
HEADERS = {"User-Agent": "Mozilla/5.0"}
ES_INDEX_NAME = Variable.get("ES_INDEX_NAME", "rag-test3")
EMBEDDING_BATCH_SIZE = int(Variable.get("EMBEDDING_BATCH_SIZE", "8"))
MAX_WORKERS = int(Variable.get("MAX_WORKERS", "3"))

# ë””ë ‰í† ë¦¬ ìƒì„±
ensure_directory(DOWNLOAD_DIR)

# ëž˜í¼ í•¨ìˆ˜ - ê³µê³ ë¬¸ URL ìˆ˜ì§‘ ë° PDF ë‹¤ìš´ë¡œë“œ
def collect_urls_wrapper(**kwargs):
    """
    LH ê³µê³ ë¬¸ URL ìˆ˜ì§‘ ë° íŒŒì¼ ë‹¤ìš´ë¡œë“œë¥¼ ìœ„í•œ ëž˜í¼ í•¨ìˆ˜
    
    ì´ í•¨ìˆ˜ëŠ” Airflow DAG ë‚´ì—ì„œ ì§ì ‘ í˜¸ì¶œë˜ì–´ LH ê³µê³ ë¬¸ í¬ë¡¤ë§ ìž‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    í¬ë¡¤ë§ ëª¨ë“ˆì—ì„œ ì •ì˜ëœ collect_lh_file_urls í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ì‹¤ì œ í¬ë¡¤ë§ ìž‘ì—…ì„ ìœ„ìž„í•©ë‹ˆë‹¤.
    ìˆ˜ì§‘ëœ PDF íŒŒì¼ì€ DOWNLOAD_DIRì— ì €ìž¥ë©ë‹ˆë‹¤.
    """
    # Airflowì˜ execution_date ì‚¬ìš© (ì‹¤í–‰ ìŠ¤ì¼€ì¤„ ë‚ ì§œ)
    execution_date = kwargs.get('execution_date', datetime.now()).date()
    print(f"ðŸ”„ ì‹¤í–‰ ë‚ ì§œ: {execution_date}")

    return collect_lh_file_urls(BASE_URL, LIST_URL, DOWNLOAD_URL, DOWNLOAD_DIR, HEADERS, execution_date)

# ëž˜í¼ í•¨ìˆ˜ - ì €ìž¥ëœ PDF íŒŒì¼ ì²˜ë¦¬ ë° ES ì ìž¬
def process_pdfs_wrapper(**kwargs):
    """
    PDF ì²˜ë¦¬ ë° Elasticsearch ì—…ë¡œë“œë¥¼ ìœ„í•œ ëž˜í¼ í•¨ìˆ˜
    
    ì´ì „ íƒœìŠ¤í¬ì—ì„œ ë‹¤ìš´ë¡œë“œí•œ PDF íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ Elasticsearchì— ì ìž¬í•©ë‹ˆë‹¤.
    ì €ìž¥ëœ PDF íŒŒì¼ì„ ì§ì ‘ ì½ì–´ ì²˜ë¦¬í•˜ë¯€ë¡œ ì¶”ê°€ì ì¸ ë„¤íŠ¸ì›Œí¬ ìš”ì²­ì´ ì—†ìŠµë‹ˆë‹¤.
    """
    from concurrent.futures import ThreadPoolExecutor
    from elasticsearch import Elasticsearch
    
    ti = kwargs['ti']
    collected_urls = ti.xcom_pull(task_ids='collect_urls')
    
    if not collected_urls:
        print("âš ï¸ ìˆ˜ì§‘ëœ ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ëž¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    # elasticsearch í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    es = get_elasticsearch_client()
    
    # PDF í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
    processor = PDFProcessor(es, batch_size=EMBEDDING_BATCH_SIZE, index_name=ES_INDEX_NAME)
    
    # ë³‘ë ¬ ì²˜ë¦¬ ì‹œìž‘
    print(f"ðŸš€ ì €ìž¥ëœ PDF íŒŒì¼ ì²˜ë¦¬ ì‹œìž‘ (ì›Œì»¤ ìˆ˜: {MAX_WORKERS})")
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = []
        for _, safe_filename, meta in collected_urls:
            # URLì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì €ìž¥ëœ íŒŒì¼ ê²½ë¡œ ì‚¬ìš©
            file_path = os.path.join(DOWNLOAD_DIR, safe_filename)
            
            # íŒŒì¼ ì¡´ìž¬ í™•ì¸
            if not os.path.exists(file_path):
                print(f"âš ï¸ íŒŒì¼ì´ ì¡´ìž¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
                continue
                
            # ìž‘ì—… ì œì¶œ
            future = executor.submit(
                process_single_pdf,
                processor=processor,
                download_dir=DOWNLOAD_DIR,
                safe_filename=safe_filename,
                meta=meta
            )
            futures.append(future)
            if future.result() is True:
                print(f"âœ… {safe_filename} ì²˜ë¦¬ ì™„ë£Œ")
            else:
                print(f"âŒ {safe_filename} ì²˜ë¦¬ ì‹¤íŒ¨")
        # ê²°ê³¼ ìˆ˜ì§‘
        success_count = sum(1 for future in futures if future.result() is True)
          
    print(f"âœ… PDF ì²˜ë¦¬ ì™„ë£Œ: {success_count}/{len(collected_urls)} ì„±ê³µ")

# DAG ì •ì˜
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'execution_timeout': timedelta(hours=1),
}

with DAG(
    'lh_notice_elasticsearch_optimized',
    default_args=default_args,
    description='LH ê³µê³ ë¬¸ í¬ë¡¤ë§ ë° Elasticsearch ì €ìž¥ (ìµœì í™” ë²„ì „)',
    schedule_interval="@daily",
    start_date=datetime(2025, 4, 28),
    catchup=True,
    tags=['lh', 'elasticsearch', 'pdf'],
) as dag:
    # ê³µê³ ë¬¸ URL ìˆ˜ì§‘ ë° PDF ë‹¤ìš´ë¡œë“œ íƒœìŠ¤í¬
    collect_task = PythonOperator(
        task_id='collect_urls',
        python_callable=collect_urls_wrapper,
    )
    
    # ì €ìž¥ëœ PDF ì²˜ë¦¬ ë° ES ì ìž¬ íƒœìŠ¤í¬
    process_task = PythonOperator(
        task_id='process_pdfs',
        python_callable=process_pdfs_wrapper,
    )
    
    # íƒœìŠ¤í¬ ì˜ì¡´ì„± ì„¤ì •
    collect_task >> process_task