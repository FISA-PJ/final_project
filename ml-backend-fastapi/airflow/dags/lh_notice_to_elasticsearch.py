# -*- coding: utf-8 -*-

"""
LH ê³µê³ ë¬¸ í¬ë¡¤ë§ ë° Elasticsearch ì €ì¥ DAG

ì´ DAGëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì£¼ìš” ê¸°ëŠ¥ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
1. LH ê³µê³  ì›¹ì‚¬ì´íŠ¸ì—ì„œ ìƒˆë¡œìš´ ê³µê³ ë¬¸ì„ í¬ë¡¤ë§
2. PDF í˜•ì‹ì˜ ê³µê³ ë¬¸ ë‹¤ìš´ë¡œë“œ
3. ë‹¤ìš´ë¡œë“œëœ PDF íŒŒì¼ì˜ ìœ íš¨ì„± ê²€ì¦
4. ìœ íš¨í•œ PDF íŒŒì¼ì„ Elasticsearchì— ì €ì¥
5. ì²˜ë¦¬ ê²°ê³¼ ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

ì‹¤í–‰ ì£¼ê¸°: ë§¤ì¼ 1íšŒ
"""

from datetime import datetime, timedelta
import subprocess
import os
from elasticsearch import Elasticsearch
import glob
import logging

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable
from airflow.hooks.base import BaseHook

# ì»¤ìŠ¤í…€ ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ ì„í¬íŠ¸
from plugins.utils.file_helpers import ensure_directory
from plugins.crawlers.lh_crawler_for_elastic import collect_lh_file_urls_and_pdf
# from plugins.crawlers.lh_crawler_for_elastic import collect_lh_notices_with_address

# ë¡œê¹… ì„¤ì •
# ë¡œê·¸ ë ˆë²¨ì„ INFOë¡œ ì„¤ì •í•˜ì—¬ ì¤‘ìš”í•œ ì‘ì—… ì§„í–‰ ìƒí™©ì„ ì¶”ì 
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

# ì½˜ì†” ì¶œë ¥ì„ ìœ„í•œ í•¸ë“¤ëŸ¬ ì„¤ì •
console_handler = logging.StreamHandler()
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)

# í¬ë¡¤ë§ ê´€ë ¨ í™˜ê²½ ì„¤ì •
# BASE_URL: LH ì²­ì•½ì„¼í„° ê¸°ë³¸ URL
# LIST_URL: ê³µê³  ëª©ë¡ í˜ì´ì§€ URL
# DOWNLOAD_URL: PDF ë‹¤ìš´ë¡œë“œ URL
BASE_URL = "https://apply.lh.or.kr"
LIST_URL = f"{BASE_URL}/lhapply/apply/wt/wrtanc/selectWrtancList.do?viewType=srch"
DOWNLOAD_URL = f"{BASE_URL}/lhapply/lhFile.do"

# íŒŒì¼ ì‹œìŠ¤í…œ ë° ì²˜ë¦¬ ê´€ë ¨ ì„¤ì •
DOWNLOAD_DIR = "/opt/airflow/downloads"  # PDF íŒŒì¼ì´ ì €ì¥ë  ë””ë ‰í† ë¦¬
HEADERS = {"User-Agent": "Mozilla/5.0"}  # í¬ë¡¤ë§ ì‹œ ì‚¬ìš©í•  User-Agent
ES_INDEX_NAME = "rag-test3"  # Elasticsearch ì¸ë±ìŠ¤ëª…
EMBEDDING_BATCH_SIZE = 8  # ì„ë² ë”© ì²˜ë¦¬ ì‹œ ë°°ì¹˜ í¬ê¸°
MAX_WORKERS = 3  # Selenium ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜

# ë‹¤ìš´ë¡œë“œ ë””ë ‰í† ë¦¬ ìƒì„±
ensure_directory(DOWNLOAD_DIR)

def collect_urls_wrapper(**kwargs):
    """
    LH ê³µê³ ë¬¸ URL ìˆ˜ì§‘ ë° íŒŒì¼ ë‹¤ìš´ë¡œë“œë¥¼ ìœ„í•œ ë˜í¼ í•¨ìˆ˜
    
    Args:
        **kwargs: Airflow context ë³€ìˆ˜ë“¤ì„ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
            - ds: ì‹¤í–‰ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹)
    
    Returns:
        list: ìˆ˜ì§‘ëœ ê³µê³  URL ëª©ë¡
    
    Note:
        - Airflowì˜ execution_dateë¥¼ ê¸°ì¤€ìœ¼ë¡œ í¬ë¡¤ë§ ìˆ˜í–‰
        - ìˆ˜ì§‘ëœ PDF íŒŒì¼ì€ DOWNLOAD_DIRì— ì €ì¥ë¨
    """
    execution_date = kwargs.get('ds')
    execution_date = datetime.strptime(execution_date, "%Y-%m-%d").date()
    logger.info(f"ğŸ”„ ì‹¤í–‰ ë‚ ì§œ: {execution_date}")

    return collect_lh_file_urls_and_pdf(BASE_URL, LIST_URL, DOWNLOAD_URL, DOWNLOAD_DIR, HEADERS, execution_date)

def get_existing_pdf_files():
    """
    Elasticsearchì—ì„œ ì´ë¯¸ ì €ì¥ëœ PDF íŒŒì¼ë“¤ì˜ ëª©ë¡ì„ ì¡°íšŒ
    
    Returns:
        set: ì´ë¯¸ ì²˜ë¦¬ëœ PDF íŒŒì¼ëª…ë“¤ì˜ ì§‘í•©
    
    Note:
        - Elasticsearch ì—°ê²°ì€ Airflow connectionì„ í†µí•´ ì„¤ì •
        - íŒŒì¼ëª… ì¤‘ë³µ ì²˜ë¦¬ë¥¼ ìœ„í•´ basenameë§Œ ì¶”ì¶œí•˜ì—¬ ì €ì¥
    """
    try:
        # Elasticsearch ì—°ê²° ì„¤ì •
        es_conn = BaseHook.get_connection("my_elasticsearch")
        es = Elasticsearch([{'host': es_conn.host, 'port': es_conn.port}])
        
        # ES ì—°ê²° ìƒíƒœ í™•ì¸
        if not es.ping():
            logger.warning("âš ï¸ Elasticsearch ì—°ê²° ì‹¤íŒ¨")
            return set()
        
        # source_file í•„ë“œë§Œ ì¡°íšŒí•˜ëŠ” ì¿¼ë¦¬ ì‹¤í–‰
        query = {
            "_source": ["source_file"],
            "query": {
                "match_all": {}
            }
        }
        
        response = es.search(index=ES_INDEX_NAME, body=query, size=10000)
        existing_files = set()
        
        # ê²°ê³¼ì—ì„œ íŒŒì¼ëª…ë§Œ ì¶”ì¶œí•˜ì—¬ ì €ì¥
        for hit in response['hits']['hits']:
            if 'source_file' in hit['_source']:
                existing_files.add(os.path.basename(hit['_source']['source_file']))
        
        return existing_files
    except Exception as e:
        logger.error(f"âš ï¸ Elasticsearch ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return set()

def validate_pdf_files(**kwargs):
    """
    ë‹¤ìš´ë¡œë“œëœ PDF íŒŒì¼ë“¤ì˜ ìœ íš¨ì„±ì„ ê²€ì‚¬í•˜ëŠ” íƒœìŠ¤í¬
    
    Args:
        **kwargs: Airflow context ë³€ìˆ˜ë“¤
    
    Returns:
        list: ìœ íš¨í•œ PDF íŒŒì¼ëª… ëª©ë¡
    
    Note:
        - ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ì€ ê±´ë„ˆëœ€
        - íŒŒì¼ í¬ê¸°, í˜•ì‹, ë‚´ìš© ë“±ì„ ê²€ì¦
        - ì²˜ë¦¬ ê²°ê³¼ëŠ” XComì„ í†µí•´ ë‹¤ìŒ íƒœìŠ¤í¬ë¡œ ì „ë‹¬
    """
    ti = kwargs['ti']
    collected_urls = ti.xcom_pull(task_ids='collect_urls_and_pdfs')
    
    if not collected_urls:
        logger.warning("âš ï¸ ìˆ˜ì§‘ëœ ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    # ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡ ì¡°íšŒ
    existing_files = get_existing_pdf_files()
    
    # ìƒˆë¡œìš´ PDF íŒŒì¼ í•„í„°ë§
    all_pdf_files = [f for f in os.listdir(DOWNLOAD_DIR) if f.endswith('.pdf')]
    new_pdf_files = [f for f in all_pdf_files if f not in existing_files]
    
    if not new_pdf_files:
        logger.info("âš ï¸ ì²˜ë¦¬í•  ìƒˆë¡œìš´ PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    logger.info(f"ğŸ”„ ì²˜ë¦¬í•  ìƒˆë¡œìš´ PDF íŒŒì¼ ìˆ˜: {len(new_pdf_files)}")
    
    # íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬ ë° ê²°ê³¼ ì¶”ì 
    processed_files = []
    valid_files = []
    
    for file_name in new_pdf_files:
        file_path = os.path.join(DOWNLOAD_DIR, file_name)
        is_valid, error_msg = validate_pdf_file(file_path)
        
        if is_valid:
            valid_files.append(file_name)
            processed_files.append({'name': file_name, 'status': 'pending'})
        else:
            processed_files.append({'name': file_name, 'status': 'failed', 'error': error_msg})
    
    # XComì„ í†µí•´ ì²˜ë¦¬ ê²°ê³¼ ì „ë‹¬
    ti.xcom_push(key='processed_files', value=processed_files)
    ti.xcom_push(key='valid_files', value=valid_files)
    
    return valid_files

def process_valid_files(**kwargs):
    """
    ìœ íš¨ì„± ê²€ì‚¬ë¥¼ í†µê³¼í•œ PDF íŒŒì¼ë“¤ì„ ì²˜ë¦¬í•˜ëŠ” íƒœìŠ¤í¬
    
    Args:
        **kwargs: Airflow context ë³€ìˆ˜ë“¤
    
    Note:
        - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
        - ì™¸ë¶€ í”„ë¡œì„¸ì„œ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
        - ì²˜ë¦¬ ê²°ê³¼ ì¶”ì  ë° ë¡œê¹…
    """
    ti = kwargs['ti']
    valid_files = ti.xcom_pull(task_ids='validate_pdfs', key='valid_files')
    processed_files = ti.xcom_pull(task_ids='validate_pdfs', key='processed_files')
    
    if not valid_files:
        logger.warning("âš ï¸ ìœ íš¨í•œ PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
    monitor_memory_usage()
    
    # í™˜ê²½ë³€ìˆ˜ë¡œ ì²˜ë¦¬í•  íŒŒì¼ ëª©ë¡ ì „ë‹¬
    os.environ['PROCESS_PDF_FILES'] = ','.join(valid_files)
    
    try:
        # ì™¸ë¶€ í”„ë¡œì„¸ì„œ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
        result = subprocess.run(
            ['python', '/opt/airflow/plugins/processors/main.py'],
            capture_output=True,
            text=True
        )

        # ì‹¤í–‰ ê²°ê³¼ ë¡œê¹…
        logger.info("ğŸ“„ [STDOUT]:\n" + result.stdout)
        if result.stderr:
            logger.error("âš ï¸ [STDERR]:\n" + result.stderr)

        if result.returncode != 0:
            raise Exception(f"Script failed with return code {result.returncode}")
        
        # ì„±ê³µ ì²˜ë¦¬
        for file in processed_files:
            if file['name'] in valid_files:
                file['status'] = 'success'
        
    except Exception as e:
        # ì‹¤íŒ¨ ì²˜ë¦¬
        for file in processed_files:
            if file['name'] in valid_files and file['status'] == 'pending':
                file['status'] = 'failed'
                file['error'] = str(e)
        raise
    
    finally:
        # ìµœì¢… ì²˜ë¦¬ ê²°ê³¼ ì „ë‹¬
        ti.xcom_push(key='final_processed_files', value=processed_files)

def cleanup_and_report(**kwargs):
    """
    ì²˜ë¦¬ ê²°ê³¼ ë¡œê¹… ë° ì •ë¦¬ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” íƒœìŠ¤í¬
    
    Args:
        **kwargs: Airflow context ë³€ìˆ˜ë“¤
    
    Note:
        - ì²˜ë¦¬ ê²°ê³¼ ìƒì„¸ ë¡œê¹…
        - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì¢… í™•ì¸
        - ì„ì‹œ íŒŒì¼ ì •ë¦¬
    """
    ti = kwargs['ti']
    processed_files = ti.xcom_pull(task_ids='process_files', key='final_processed_files')
    
    # ì²˜ë¦¬ ê²°ê³¼ ë¡œê¹…
    log_processing_results(processed_files)
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì¢… í™•ì¸
    monitor_memory_usage()
    
    # ì„ì‹œ íŒŒì¼ ì •ë¦¬
    cleanup_temp_files()

def monitor_memory_usage():
    """
    í˜„ì¬ í”„ë¡œì„¸ìŠ¤ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ëª¨ë‹ˆí„°ë§í•˜ê³  ë¡œê¹…
    
    Note:
        - psutilì„ ì‚¬ìš©í•˜ì—¬ í˜„ì¬ í”„ë¡œì„¸ìŠ¤ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
        - MB ë‹¨ìœ„ë¡œ ë³€í™˜í•˜ì—¬ ë¡œê¹…
    """
    import psutil
    process = psutil.Process(os.getpid())
    memory_usage = process.memory_info().rss / 1024 / 1024  # MB ë‹¨ìœ„ë¡œ ë³€í™˜
    logger.info(f"ğŸ“Š í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage:.2f} MB")

def log_processing_results(processed_files):
    """
    íŒŒì¼ ì²˜ë¦¬ ê²°ê³¼ë¥¼ ìƒì„¸í•˜ê²Œ ë¡œê¹…
    
    Args:
        processed_files (list): ì²˜ë¦¬ëœ íŒŒì¼ë“¤ì˜ ìƒíƒœ ì •ë³´
    
    Note:
        - ì „ì²´ ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½
        - ì‹¤íŒ¨í•œ íŒŒì¼ë“¤ì— ëŒ€í•œ ìƒì„¸ ì •ë³´ ë¡œê¹…
    """
    if not processed_files:
        logger.warning("ì²˜ë¦¬ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
        
    success_count = len([f for f in processed_files if f['status'] == 'success'])
    failed_count = len([f for f in processed_files if f['status'] == 'failed'])
    
    logger.info(f"""
        ğŸ“‹ ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½:
        - ì „ì²´ íŒŒì¼ ìˆ˜: {len(processed_files)}
        - ì„±ê³µ: {success_count}
        - ì‹¤íŒ¨: {failed_count}
        """)
    
    if failed_count > 0:
        logger.error("âŒ ì‹¤íŒ¨í•œ íŒŒì¼ë“¤:")
        for file in processed_files:
            if file['status'] == 'failed':
                logger.error(f"- {file['name']}: {file['error']}")

def validate_pdf_file(file_path):
    """
    ê°œë³„ PDF íŒŒì¼ì˜ ìœ íš¨ì„±ì„ ê²€ì‚¬
    
    Args:
        file_path (str): ê²€ì‚¬í•  PDF íŒŒì¼ì˜ ê²½ë¡œ
    
    Returns:
        tuple: (bool, str) - (ìœ íš¨ì„± ì—¬ë¶€, ì˜¤ë¥˜ ë©”ì‹œì§€)
    
    Note:
        - íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        - íŒŒì¼ í™•ì¥ì ê²€ì¦
        - íŒŒì¼ í¬ê¸° í™•ì¸
        - PDF í˜•ì‹ ê²€ì¦
    """
    try:
        if not os.path.exists(file_path):
            return False, "íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"
        
        if not file_path.lower().endswith('.pdf'):
            return False, "PDF íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤"
        
        if os.path.getsize(file_path) == 0:
            return False, "ë¹ˆ íŒŒì¼ì…ë‹ˆë‹¤"
        
        # PDF íŒŒì¼ í˜•ì‹ ê²€ì‚¬
        import PyPDF2
        with open(file_path, 'rb') as f:
            try:
                PyPDF2.PdfReader(f)
                return True, None
            except:
                return False, "ìœ íš¨í•˜ì§€ ì•Šì€ PDF íŒŒì¼ì…ë‹ˆë‹¤"
                
    except Exception as e:
        return False, f"íŒŒì¼ ê²€ì‚¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

def cleanup_temp_files():
    """
    ì²˜ë¦¬ ì™„ë£Œ í›„ ì„ì‹œ íŒŒì¼ë“¤ì„ ì •ë¦¬
    
    Note:
        - ì„ì‹œ íŒŒì¼ íŒ¨í„´: *.tmp, *.temp, *.log
        - ì‚­ì œ ì‹¤íŒ¨ ì‹œ ê²½ê³  ë¡œê·¸ ê¸°ë¡
    """
    temp_patterns = ['*.tmp', '*.temp', '*.log']
    for pattern in temp_patterns:
        for file_path in glob.glob(os.path.join(DOWNLOAD_DIR, pattern)):
            try:
                os.remove(file_path)
                logger.info(f"ğŸ—‘ï¸ ì„ì‹œ íŒŒì¼ ì‚­ì œ: {file_path}")
            except Exception as e:
                logger.warning(f"âš ï¸ ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {file_path} - {str(e)}")

def run_main_script(**kwargs):
    """
    ë©”ì¸ ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        **kwargs: Airflow context ë³€ìˆ˜ë“¤
    
    Note:
        - ìƒˆë¡œìš´ PDF íŒŒì¼ í•„í„°ë§
        - ì™¸ë¶€ í”„ë¡œì„¸ì„œ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
        - ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ì²˜ë¦¬ ë° ë¡œê¹…
    """
    ti = kwargs['ti']
    collected_urls = ti.xcom_pull(task_ids='collect_urls_and_pdfs')
    
    if not collected_urls:
        logger.info("âš ï¸ ìˆ˜ì§‘ëœ ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    # ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ í•„í„°ë§
    existing_files = get_existing_pdf_files()
    all_pdf_files = [f for f in os.listdir(DOWNLOAD_DIR) if f.endswith('.pdf')]
    new_pdf_files = [f for f in all_pdf_files if f not in existing_files]
    
    if not new_pdf_files:
        logger.info("âš ï¸ ì²˜ë¦¬í•  ìƒˆë¡œìš´ PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    logger.info(f"ğŸ”„ ì²˜ë¦¬í•  ìƒˆë¡œìš´ PDF íŒŒì¼ ìˆ˜: {len(new_pdf_files)}")
    
    # í™˜ê²½ë³€ìˆ˜ë¡œ ì²˜ë¦¬í•  íŒŒì¼ ëª©ë¡ ì „ë‹¬
    os.environ['PROCESS_PDF_FILES'] = ','.join(new_pdf_files)
    
    try:
        # ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
        result = subprocess.run(
            ['python', '/opt/airflow/plugins/processors/main.py'],
            capture_output=True,
            text=True,
            check=True
        )
        logger.info(result.stdout)
        
    except subprocess.CalledProcessError as e:
        logger.error(f"Script failed with error: {e.stderr}")
        raise
    except Exception as e:
        logger.error(f"Unexpected error: {str(e)}")
        raise

# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'airflow',  # DAG ì†Œìœ ì
    'depends_on_past': False,  # ì´ì „ ì‹¤í–‰ ê²°ê³¼ì— ì˜ì¡´í•˜ì§€ ì•ŠìŒ
    'email_on_failure': False,  # ì‹¤íŒ¨ ì‹œ ì´ë©”ì¼ ì•Œë¦¼ ë¹„í™œì„±í™”
    'email_on_retry': False,  # ì¬ì‹œë„ ì‹œ ì´ë©”ì¼ ì•Œë¦¼ ë¹„í™œì„±í™”
    'retries': 3,  # ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ íšŸìˆ˜
    'retry_delay': timedelta(minutes=5),  # ì¬ì‹œë„ ê°„ê²©
    'execution_timeout': timedelta(hours=5),  # ìµœëŒ€ ì‹¤í–‰ ì‹œê°„
}

# DAG ì •ì˜
with DAG(
    'LH_Notice_To_ElasticSearch',  # DAG ID
    default_args=default_args,
    description='LH ê³µê³ ë¬¸ í¬ë¡¤ë§ ë° Elasticsearch ì €ì¥',
    schedule="@daily",  # ë§¤ì¼ ì‹¤í–‰
    start_date=datetime(2025, 1, 1),  # ì‹œì‘ ë‚ ì§œ
    catchup=True,  # ê³¼ê±° ë‚ ì§œì— ëŒ€í•œ ë°±í•„ í™œì„±í™”
    tags=['lh', 'elasticsearch', 'pdf'],  # DAG íƒœê·¸
) as dag:
    # Task 1: ê³µê³ ë¬¸ URL ìˆ˜ì§‘ ë° PDF ë‹¤ìš´ë¡œë“œ
    collect_task = PythonOperator(
        task_id='collect_urls_and_pdfs',
        python_callable=collect_urls_wrapper,
    )

    # Task 2: PDF íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬
    validate_task = PythonOperator(
        task_id='validate_pdfs',
        python_callable=validate_pdf_files,
    )

    # Task 3: PDF ì²˜ë¦¬ ë° ES ì ì¬
    process_task = PythonOperator(
        task_id='process_files',
        python_callable=process_valid_files,
    )

    # Task 4: ì •ë¦¬ ë° ê²°ê³¼ ë³´ê³ 
    cleanup_task = PythonOperator(
        task_id='cleanup_and_report',
        python_callable=cleanup_and_report,
    )
    
    # íƒœìŠ¤í¬ ì˜ì¡´ì„± ì„¤ì • (ì‹¤í–‰ ìˆœì„œ ì •ì˜)
    collect_task >> validate_task >> process_task >> cleanup_task