# -*- coding: utf-8 -*-
from elasticsearch import Elasticsearch, helpers
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator

# process_existing_pdfs í•¨ìˆ˜ ì •ì˜
def process_existing_pdfs(**kwargs):
    """
    ì´ë¯¸ ë‹¤ìš´ë¡œë“œëœ PDF íŒŒì¼ë“¤ì„ ì²˜ë¦¬í•˜ê³  Elasticsearchì— ìƒ‰ì¸í•˜ëŠ” í•¨ìˆ˜
    
    ë‹¤ìš´ë¡œë“œ ë””ë ‰í† ë¦¬ì—ì„œ ê¸°ì¡´ì— ìˆ˜ì§‘ëœ PDF íŒŒì¼ë“¤ì„ ëª¨ë‘ ì½ì–´ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    from airflow.models import Variable
    import os
    import re
    # PDFProcessor í´ë˜ìŠ¤ ì„í¬íŠ¸
    from plugins.processors.pdf_processors import PDFProcessor
    from plugins.processors.es_uploaders import get_elasticsearch_client
    from plugins.processors.es_uploaders import process_single_pdf
    
    # í™˜ê²½ ë³€ìˆ˜ ë° ìƒìˆ˜ ê°€ì ¸ì˜¤ê¸°
    DOWNLOAD_DIR = "/opt/airflow/downloads"
    EMBEDDING_BATCH_SIZE = int(Variable.get("EMBEDDING_BATCH_SIZE", "8"))
    ES_INDEX_NAME = Variable.get("ES_INDEX_NAME", "rag-test3")
    MAX_WORKERS = int(Variable.get("MAX_WORKERS", "3"))
    
    # ë‹¤ìš´ë¡œë“œ ë””ë ‰í† ë¦¬ í™•ì¸
    if not os.path.exists(DOWNLOAD_DIR):
        print(f"âš ï¸ ë‹¤ìš´ë¡œë“œ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {DOWNLOAD_DIR}")
        return
    
    # PDF íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    pdf_files = [f for f in os.listdir(DOWNLOAD_DIR) if f.lower().endswith('.pdf')]
    if not pdf_files:
        print("âš ï¸ ì²˜ë¦¬í•  PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ” ì´ {len(pdf_files)}ê°œì˜ PDF íŒŒì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
    
    # Elasticsearch í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    try:
        es = get_elasticsearch_client()
        processor = PDFProcessor(es, batch_size=EMBEDDING_BATCH_SIZE, index_name=ES_INDEX_NAME)
    except Exception as e:
        print(f"âŒ Elasticsearch ì—°ê²° ì‹¤íŒ¨: {str(e)}")
        return
    
    # íŒŒì¼ëª…ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (íŒ¨í„´: {wrtan_no}_{pub_date}_{filename}.pdf)
    file_data = []
    for file in pdf_files:
        try:
            # ì •ê·œì‹ìœ¼ë¡œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            match = re.match(r'([^_]+)_(\d+)_(.+)\.pdf', file)
            if match:
                wrtan_no, pub_date, short_name = match.groups()
                meta = {
                    'wrtan_no': wrtan_no,
                    'pub_date': pub_date,
                    'filename': f"{short_name}.pdf"
                }
                file_data.append((file, meta))
            else:
                # íŒ¨í„´ì´ ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ìƒì„±
                meta = {
                    'wrtan_no': 'unknown',
                    'pub_date': '00000000',
                    'filename': file
                }
                file_data.append((file, meta))
        except Exception as e:
            print(f"âš ï¸ íŒŒì¼ {file}ì˜ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
    
    # ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ í™•ì¸ (Elasticsearchì— ì¿¼ë¦¬í•˜ì—¬ í™•ì¸)
    processed_files = set()
    try:
        query = {
            "size": 10000,  # ì¶©ë¶„íˆ í° ê°’ìœ¼ë¡œ ì„¤ì •
            "query": {"match_all": {}},
            "_source": ["source_pdf"]
        }
        response = es.search(index=ES_INDEX_NAME, body=query)
        for hit in response["hits"]["hits"]:
            processed_files.add(hit["_source"]["source_pdf"])
        
        print(f"ğŸ“Š ì´ë¯¸ ì²˜ë¦¬ëœ PDF íŒŒì¼: {len(processed_files)}ê°œ")
    except Exception as e:
        print(f"âš ï¸ ì²˜ë¦¬ëœ íŒŒì¼ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    # ì²˜ë¦¬í•  íŒŒì¼ í•„í„°ë§ (ì•„ì§ ì²˜ë¦¬ë˜ì§€ ì•Šì€ íŒŒì¼ë§Œ)
    to_process = [(file, meta) for file, meta in file_data if file not in processed_files]
    print(f"ğŸš€ ì²˜ë¦¬í•  ìƒˆ PDF íŒŒì¼: {len(to_process)}ê°œ")
    
    if not to_process:
        print("â„¹ï¸ ëª¨ë“  PDF íŒŒì¼ì´ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return
    
    # ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = []
        for file, meta in to_process:
            future = executor.submit(
                process_single_pdf,
                processor=processor,
                download_dir=DOWNLOAD_DIR,
                safe_filename=file,
                meta=meta
            )
            futures.append(future)
        
        # ê²°ê³¼ ìˆ˜ì§‘
        success_count = sum(1 for future in futures if future.result() is True)
    
    print(f"âœ… PDF ì²˜ë¦¬ ì™„ë£Œ: {success_count}/{len(to_process)} ì„±ê³µ")

# DAG ì •ì˜
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'execution_timeout': timedelta(hours=2),  # ë” ê¸´ ì‹¤í–‰ ì‹œê°„ í—ˆìš©
}

with DAG(
    'Process_Existing_Pdf_Files_To_ElasticSearch',
    default_args=default_args,
    description='ê¸°ì¡´ ì €ì¥ëœ PDF íŒŒì¼ ì²˜ë¦¬ ë° Elasticsearch ì ì¬',
    schedule=None,  # ìˆ˜ë™ ì‹¤í–‰ìš© (ì¼íšŒì„± ì‘ì—…)
    start_date=datetime(2025, 5, 1),
    catchup=False,
    tags=['elasticsearch', 'pdf', 'one-time'],
) as dag:
    # ê¸°ì¡´ PDF ì²˜ë¦¬ íƒœìŠ¤í¬
    process_existing_task = PythonOperator(
        task_id='process_existing_pdfs',
        python_callable=process_existing_pdfs,
    )