import os
from dotenv import load_dotenv
from langchain.schema import Document
from elasticsearch import Elasticsearch
from langchain_elasticsearch import ElasticsearchStore
import logging
from elasticsearch.exceptions import NotFoundError, TransportError

from plugins.processors.loader import Parser2Markdown
from plugins.processors.chunker import HeaderSplitter, SemanticSplitter
from plugins.processors.embedding import BgeM3Embedding

from datetime import datetime
import json

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
logger.handlers = []  # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
handler = logging.StreamHandler()  # stdoutìœ¼ë¡œ ì¶œë ¥
formatter = logging.Formatter("%(levelname)s - %(message)s")
handler.setFormatter(formatter)
logger.addHandler(handler)

# í…ŒìŠ¤íŠ¸ ë¡œê·¸
# logger.info("main.py ì‹¤í–‰ ì‹œì‘")

# í™˜ê²½ ì„¤ì •
load_dotenv()

# ìˆ˜ì •ëœ ì½”ë“œ - ë””ë ‰í† ë¦¬ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
target_dir = "/opt/airflow/downloads/normal_dag"
if os.path.exists(target_dir):
    os.chdir(target_dir)
else:
    # ë””ë ‰í† ë¦¬ ìƒì„± ë˜ëŠ” ë‹¤ë¥¸ ì²˜ë¦¬
    os.makedirs(target_dir, exist_ok=True)
    os.chdir(target_dir)

# Elasticsearch ì—°ê²°
UPSTAGE_API_KEY = ''
preprocessor = Parser2Markdown(UPSTAGE_API_KEY)

embeddings = BgeM3Embedding()
header_splitter = HeaderSplitter()
second_splitter = SemanticSplitter(embeddings)
es = Elasticsearch("http://airflow-elasticsearch:9200")
index_name = "test-0524-tmp"

vectorstore = ElasticsearchStore(
    index_name=index_name,
    embedding=embeddings,
    es_connection=es,
)

# # ì´ë¯¸ íŒŒì‹±ëœ íŒŒì¼ ì½ì–´ì˜¤ëŠ” ê²½ìš°
# def read_md_file(file_path):
#     with open(file_path, 'r', encoding='utf-8') as f:
#         return f.read()


def delete_documents_by_apt_code(es: Elasticsearch, index: str, apt_code: str):
    if not es.indices.exists(index=index):
        logger.info(f"âš ï¸ ì¸ë±ìŠ¤ '{index}'ê°€ ì¡´ì¬í•˜ì§€ ì•Šì•„ ì‚­ì œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return

    query = {"query": {"term": {"metadata.apt_code": apt_code}}}

    try:
        response = es.delete_by_query(index=index, body=query)
        deleted = response.get(deleted, 0)
        logger.info(
            f"ğŸ—‘ï¸ apt_code={apt_code} ë¬¸ì„œ {deleted}ê±´ ì‚­ì œ ì™„ë£Œ"
            if deleted
            else f"â„¹ï¸ apt_code={apt_code} ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ"
        )

    except NotFoundError as e:
        logger.info(f"ğŸš« ë¬¸ì„œ ì—†ìŒ: {e}")
    except TransportError as e:
        logger.info(f"ğŸš¨ ìš”ì²­ ì‹¤íŒ¨: {e.error}, ìƒíƒœ ì½”ë“œ: {e.status_code}")
    except Exception as e:
        logger.info(f"â— ì˜ˆì™¸ ë°œìƒ: {e}")


def process_pdfs():
    # ì‹¤í–‰ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°
    execution_date = os.getenv(
        "AIRFLOW_EXECUTION_DATE", datetime.now().strftime("%Y-%m-%d")
    )

    # ES ì—…ë¡œë“œ ì‹¤íŒ¨ ê¸°ë¡ ë””ë ‰í„°ë¦¬ ìƒì„±
    failed_records_dir = "esupload_failed_records"
    os.makedirs(failed_records_dir, exist_ok=True)

    # ë‚ ì§œë³„ ì‹¤íŒ¨ ê¸°ë¡ íŒŒì¼ ê²½ë¡œ
    failed_file_path = os.path.join(
        failed_records_dir, f"{execution_date}_failed_files.json"
    )

    # ì²˜ë¦¬í•  PDF íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    process_files_json = os.getenv("PROCESS_PDF_FILES", "[]")
    process_files = json.loads(process_files_json)

    # ES ì ì¬ ì‹¤íŒ¨í•œ íŒŒì¼ ëª©ë¡ ë‹´ì„ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    failed_files = []

    # íŒŒì¼ì´ ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš° ê²½ê³  ë¡œê·¸ ì¶œë ¥
    if not process_files or process_files[0] == "":
        logger.warning("ì²˜ë¦¬í•  PDF íŒŒì¼ì´ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return

    for file_name in process_files:
        file_path = os.path.join("/opt/airflow/downloads/normal_dag", file_name)
        try:
            if not os.path.exists(file_path):
                logger.error(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
                continue

            logger.info(
                f"======== ğŸš©{file_name} íŒŒì¼ì— ëŒ€í•œ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. ========"
            )

            ## 1. ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ (Elasticsearch ì¿¼ë¦¬)
            apt_code = file_name.split("_")[0]  # file_name
            delete_documents_by_apt_code(es, index_name, apt_code)

            ## 2. PDFë¬¸ íŒŒì‹± ë° ë§ˆí¬ë‹¤ìš´ í˜•íƒœë¡œ ë³€í™˜
            html_contents = preprocessor.pdf_upstageparser(file_name)
            # html_contents = preprocessor.pdf_openparse(file_name)
            # html_contents = read_md_file(file_name)
            markdown_texts = preprocessor.html_to_markdown_with_tables(html_contents)

            doc = Document(
                page_content=markdown_texts, metadata={"source_pdf": file_name}
            )

            ## 3. 1ì°¨ í—¤ë” ê¸°ë°˜ ì²­í¬
            header_chunks = header_splitter.split_documents([doc])

            ## 4. 2ì°¨ ì˜ë¯¸ ê¸°ë°˜ ì²­í¬
            final_documents = second_splitter.split_documents(header_chunks)

            ## 5. ì¼ê´„ ì„ë² ë”© + ë²¡í„° ì €ì¥
            vectorstore.add_documents(final_documents)
            logger.info(
                f"-Ë‹Ëâœ„â”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆ [ì™„ë£Œ] {len(final_documents)}ê°œ ë¬¸ì„œ Elasticsearchì— ì ì¬ë˜ì—ˆìŠµë‹ˆë‹¤. â”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆâ”ˆ\n"
            )

        except Exception as e:
            logger.error(f"{file_name} ES ì ì¬ ì‹¤íŒ¨: {str(e)}")
            logger.error(f"íŒŒì¼ {file_name} ì„ ì ì¬ ì‹¤íŒ¨ ëª©ë¡ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.")
            failed_files.append(file_name)
            continue  # ë‹¤ìŒ íŒŒì¼ë¡œ ê³„ì† ì§„í–‰

    # ì‹¤íŒ¨í•œ íŒŒì¼ ëª©ë¡ ì €ì¥
    if failed_files:
        with open(failed_file_path, "w") as f:
            json.dump(
                {
                    "date": execution_date,  # ì‹¤í–‰ ë‚ ì§œ
                    "failed_files": failed_files,  # ì‹¤íŒ¨í•œ PDF íŒŒì¼ëª… ëª©ë¡
                },
                f,
                indent=2,
            )
        logger.info(f"ES ì ì¬ ì‹¤íŒ¨ íŒŒì¼ ëª©ë¡ì„ {failed_file_path}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    # logger.info("main.py ì‹¤í–‰ ì™„ë£Œ")


if __name__ == "__main__":
    process_pdfs()
