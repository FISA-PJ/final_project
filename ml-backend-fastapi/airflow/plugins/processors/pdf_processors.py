import torch
import torch.nn.functional as F
from torch import Tensor
from typing import List, Optional
from datetime import datetime
from transformers import AutoTokenizer, AutoModel
from langchain.text_splitter import RecursiveCharacterTextSplitter
from elasticsearch import Elasticsearch, helpers
import openparse

# PDF ì²˜ë¦¬ í´ë˜ìŠ¤ ìµœì í™”
class PDFProcessor:
    """
    PDF ë¬¸ì„œ ì²˜ë¦¬ ë° ì„ë² ë”© ìƒì„± í´ë˜ìŠ¤
    
    ì´ í´ë˜ìŠ¤ëŠ” PDF ë¬¸ì„œë¥¼ íŒŒì‹±í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³ , ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• í•œ í›„,
    ê° ì²­í¬ì— ëŒ€í•œ ì„ë² ë”© ë²¡í„°ë¥¼ ìƒì„±í•˜ì—¬ Elasticsearchì— ì €ì¥í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
    ëª¨ë“  ì²˜ë¦¬ ê³¼ì •ì€ ë°°ì¹˜ ì²˜ë¦¬ ë°©ì‹ìœ¼ë¡œ ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
    
    Attributes:
    -----------
    es : Elasticsearch
        Elasticsearch í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤
    index_name : str
        ì„ë² ë”©ì´ ì €ì¥ë  Elasticsearch ì¸ë±ìŠ¤ ì´ë¦„
    batch_size : int
        ë°°ì¹˜ ì²˜ë¦¬ ì‹œ ì²­í¬ ë‹¨ìœ„ì˜ í¬ê¸°
    model_name : str
        ì„ë² ë”© ìƒì„±ì— ì‚¬ìš©í•  ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì˜ ì´ë¦„
    _tokenizer : AutoTokenizer
        í…ìŠ¤íŠ¸ í† í°í™”ë¥¼ ìœ„í•œ í† í¬ë‚˜ì´ì € ì¸ìŠ¤í„´ìŠ¤ (ì§€ì—° ì´ˆê¸°í™”)
    _model : AutoModel
        ì„ë² ë”© ìƒì„±ì„ ìœ„í•œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ (ì§€ì—° ì´ˆê¸°í™”)
    """
    
    def __init__(
        self, 
        es: Elasticsearch, 
        batch_size: int, 
        index_name: str, 
        model_name: str = 'intfloat/multilingual-e5-small'
    ):
        """
        PDFProcessor í´ë˜ìŠ¤ ì´ˆê¸°í™”
        
        Parameters:
        -----------
        es : Elasticsearch
            Elasticsearch í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤
        batch_size : int
            ë°°ì¹˜ ì²˜ë¦¬ ì‹œ ì²­í¬ ë‹¨ìœ„ì˜ í¬ê¸°
        index_name : str
            ì„ë² ë”©ì´ ì €ì¥ë  Elasticsearch ì¸ë±ìŠ¤ ì´ë¦„
        model_name : str, optional
            ì„ë² ë”© ìƒì„±ì— ì‚¬ìš©í•  ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì˜ ì´ë¦„, ê¸°ë³¸ê°’ì€ 'intfloat/multilingual-e5-small'
        """
        self.es = es
        self.index_name = index_name
        self.batch_size = batch_size
        # í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë“œëŠ” ì§€ì—° ì´ˆê¸°í™” (í•„ìš”í•  ë•Œë§Œ ë¡œë“œ)
        self._tokenizer = None
        self._model = None
        self.model_name = model_name
        
        # ğŸ“ìˆ˜ì • - device ì„¤ì •
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"â–¶ï¸ Using device: {self.device}")
        
        # ì¸ë±ìŠ¤ ì¡´ì¬ í™•ì¸ ë° ìƒì„±
        self._ensure_index_exists()
    
    @property
    def tokenizer(self):
        """
        í† í¬ë‚˜ì´ì € ì§€ì—° ì´ˆê¸°í™” í”„ë¡œí¼í‹°
        
        í† í¬ë‚˜ì´ì €ë¥¼ ì²˜ìŒ ì‚¬ìš©í•  ë•Œë§Œ ë¡œë“œí•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ìµœì í™”í•©ë‹ˆë‹¤.
        
        Returns:
        --------
        AutoTokenizer
            ì´ˆê¸°í™”ëœ í† í¬ë‚˜ì´ì € ì¸ìŠ¤í„´ìŠ¤
        """
        if self._tokenizer is None:
            self._tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        return self._tokenizer
    
    @property
    def model(self):
        """
        ëª¨ë¸ ì§€ì—° ì´ˆê¸°í™” í”„ë¡œí¼í‹°
        
        ëª¨ë¸ì„ ì²˜ìŒ ì‚¬ìš©í•  ë•Œë§Œ ë¡œë“œí•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ìµœì í™”í•©ë‹ˆë‹¤.
        
        Returns:
        --------
        AutoModel
            ì´ˆê¸°í™”ëœ ì„ë² ë”© ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
        """
        if self._model is None:
            # ìˆ˜ì •: to_empty() ì‚¬ìš© í›„ load_state_dict()ë¡œ ëª¨ë¸ ë¡œë“œ
            self._model = AutoModel.from_pretrained(self.model_name)
            if torch.cuda.is_available():
                # ë¨¼ì € CPUì— ëª¨ë¸ ë¡œë“œ í›„ GPUë¡œ ì´ë™
                self._model = self._model.to(self.device)
        return self._model
    
    def _ensure_index_exists(self):
        """
        í•„ìš”í•œ Elasticsearch ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ìƒì„±
        
        ì´ ë©”ì„œë“œëŠ” í´ë˜ìŠ¤ ì´ˆê¸°í™” ì‹œ ìë™ìœ¼ë¡œ í˜¸ì¶œë˜ì–´ í•„ìš”í•œ ì¸ë±ìŠ¤ê°€ 
        Elasticsearchì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³ , ì—†ì„ ê²½ìš° ì ì ˆí•œ ë§¤í•‘ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
        íŠ¹íˆ ì„ë² ë”©ì„ ìœ„í•œ dense_vector í•„ë“œë¥¼ ì ì ˆíˆ êµ¬ì„±í•©ë‹ˆë‹¤.
        """
        if not self.es.indices.exists(index=self.index_name):
            # ì¸ë±ìŠ¤ ìƒì„± (ì„ë² ë”© í•„ë“œ ë§¤í•‘ í¬í•¨)
            mapping = {
                "mappings": {
                    "properties": {
                        "content": {"type": "text"},
                        "embedding": {
                            "type": "dense_vector",
                            "dims": 384,  # multilingual-e5-small ëª¨ë¸ì˜ ì„ë² ë”© ì°¨ì›
                            "index": True,
                            "similarity": "cosine"
                        },
                        "source_pdf": {"type": "keyword"},
                        "chunk_id": {"type": "integer"}
                    }
                }
            }
            self.es.indices.create(index=self.index_name, body=mapping)
            print(f"âœ… ì¸ë±ìŠ¤ '{self.index_name}' ìƒì„± ì™„ë£Œ")
    
    def pdf_parser(self, file_path: str) -> str:
        """
        PDF ë¬¸ì„œ íŒŒì‹±í•˜ì—¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        
        openparse ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Parameters:
        -----------
        file_path : str
            íŒŒì‹±í•  PDF íŒŒì¼ì˜ ê²½ë¡œ
            
        Returns:
        --------
        str
            PDFì—ì„œ ì¶”ì¶œëœ ì „ì²´ í…ìŠ¤íŠ¸
            
        Raises:
        -------
        Exception
            PDF íŒŒì‹± ì¤‘ ë°œìƒí•˜ëŠ” ëª¨ë“  ì˜ˆì™¸
        """
        try:
            parser = openparse.DocumentParser()
            parsed_docs = parser.parse(file_path)
            # ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ìœ¼ë¡œ ìµœì í™”
            text_list = [node.text for node in parsed_docs.nodes if hasattr(node, 'text')]
            return '\n'.join(text_list)
        except Exception as e:
            print(f"âš ï¸ PDF íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
            raise
    
    def text_chunking(self, text: str, chunk_size: int = 2000, chunk_overlap: int = 200) -> List[str]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì ì ˆí•œ í¬ê¸°ë¡œ ë¶„í• 
        
        ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ê¸° ì‰¬ìš´ í¬ê¸°ì˜ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
        RecursiveCharacterTextSplitterë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ë§¥ì„ ìœ ì§€í•˜ë©´ì„œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•©ë‹ˆë‹¤.
        
        Parameters:
        -----------
        text : str
            ë¶„í• í•  ì›ë³¸ í…ìŠ¤íŠ¸
        chunk_size : int, optional
            ê° ì²­í¬ì˜ ìµœëŒ€ ë¬¸ì ìˆ˜, ê¸°ë³¸ê°’ì€ 2000
        chunk_overlap : int, optional
            ì—°ì†ëœ ì²­í¬ ê°„ì˜ ì¤‘ë³µ ë¬¸ì ìˆ˜, ê¸°ë³¸ê°’ì€ 200
            
        Returns:
        --------
        List[str]
            ë¶„í• ëœ í…ìŠ¤íŠ¸ ì²­í¬ì˜ ë¦¬ìŠ¤íŠ¸
        """
        if not text.strip():
            return []
            
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        chunks = splitter.split_text(text)
        print(f"â˜ºï¸ ì²­í‚¹ ê²°ê³¼: {len(chunks)}ê°œ ì²­í¬ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return chunks
    
    def average_pool(self, last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:
        """
        í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±ì„ ìœ„í•œ í‰ê·  í’€ë§
        
        í† í° ì„ë² ë”©ì„ í‰ê·  í’€ë§í•˜ì—¬ í…ìŠ¤íŠ¸ ì„ë² ë”© ë²¡í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        ë§ˆìŠ¤í‚¹ëœ í† í°(íŒ¨ë”©)ì€ í‰ê·  ê³„ì‚°ì—ì„œ ì œì™¸ë©ë‹ˆë‹¤.
        
        Parameters:
        -----------
        last_hidden_states : Tensor
            ëª¨ë¸ì˜ ë§ˆì§€ë§‰ ì€ë‹‰ì¸µ ì¶œë ¥
        attention_mask : Tensor
            íŒ¨ë”© í† í°ì„ ë§ˆìŠ¤í‚¹í•˜ê¸° ìœ„í•œ ì–´í…ì…˜ ë§ˆìŠ¤í¬
            
        Returns:
        --------
        Tensor
            í‰ê·  í’€ë§ëœ ì„ë² ë”© ë²¡í„°
        """
        last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)
        return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]
    
    def text_embedding(self, chunks: List[str]) -> List[List[float]]:
        """
        í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
        
        í…ìŠ¤íŠ¸ ì²­í¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ê° ì²­í¬ì— ëŒ€í•œ ì„ë² ë”© ë²¡í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        ë°°ì¹˜ ì²˜ë¦¬ ë°©ì‹ìœ¼ë¡œ íš¨ìœ¨ì ìœ¼ë¡œ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.
        
        Parameters:
        -----------
        chunks : List[str]
            ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ì²­í¬ ë¦¬ìŠ¤íŠ¸
            
        Returns:
        --------
        List[List[float]]
            ìƒì„±ëœ ì„ë² ë”© ë²¡í„°ì˜ ë¦¬ìŠ¤íŠ¸ (ê° ë²¡í„°ëŠ” float ë¦¬ìŠ¤íŠ¸)
        """
        if not chunks:
            return []
            
        # ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
        all_embeddings = []
        for i in range(0, len(chunks), self.batch_size):
            batch = chunks[i:i+self.batch_size]
            
            # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê°œì„ ì„ ìœ„í•´ with torch.no_grad() ì¶”ê°€
            with torch.no_grad():
                batch_dict = self.tokenizer(
                    batch,
                    max_length=512,
                    padding=True,
                    truncation=True,
                    return_tensors='pt'
                )
                
                # ëª¨ë¸ ì¶”ë¡  - í…ì„œë¥¼ deviceë¡œ ì´ë™ (ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
                if torch.cuda.is_available():
                    input_ids = batch_dict['input_ids'].to(self.device)
                    attention_mask = batch_dict['attention_mask'].to(self.device)
                    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                else:
                    outputs = self.model(**batch_dict)

                # í‰ê·  í’€ë§ê³¼ ì •ê·œí™”
                if torch.cuda.is_available():
                    embeddings = self.average_pool(outputs.last_hidden_state, attention_mask)
                else:
                    embeddings = self.average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])

                embeddings = F.normalize(embeddings, p=2, dim=1)

                # CPUë¡œ ì´ë™í•˜ì—¬ ì €ì¥
                all_embeddings.append(embeddings.cpu())
                 
        # ê²°ê³¼ ê²°í•©
        result = torch.cat(all_embeddings, dim=0).tolist() if all_embeddings else []
        print(f"â˜ºï¸ ì„ë² ë”© ê²°ê³¼: {len(result)}ê°œì˜ ì„ë² ë”©ì´ ëª¨ë‘ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤")
        return result
    
    def upload_embeddings_to_es(
        self, 
        chunks: List[str], 
        embeddings: List[List[float]], 
        safe_filename: str, 
        pdf_name: str
    ):
        """
        ì„ë² ë”© ë²¡í„°ë¥¼ Elasticsearchì— ì—…ë¡œë“œ
        
        ìƒì„±ëœ ì„ë² ë”© ë²¡í„°ì™€ ê´€ë ¨ ë©”íƒ€ë°ì´í„°ë¥¼ Elasticsearchì— ë²Œí¬ ì—…ë¡œë“œí•©ë‹ˆë‹¤.
        
        Parameters:
        -----------
        chunks : List[str]
            í…ìŠ¤íŠ¸ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        embeddings : List[List[float]]
            ê° ì²­í¬ì— í•´ë‹¹í•˜ëŠ” ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
        safe_filename : str
            ì €ì¥ëœ PDF íŒŒì¼ì˜ ì•ˆì „í•œ íŒŒì¼ëª… (íŒŒì¼ ì‹ë³„ìë¡œ ì‚¬ìš©)
        pdf_name : str
            ì›ë³¸ PDF íŒŒì¼ëª…
            
        Notes:
        ------
        ë²Œí¬ ì—…ë¡œë“œ ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ Elasticsearchì— ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        ê° ë¬¸ì„œì—ëŠ” ì²­í¬ ë‚´ìš©, ì„ë² ë”© ë²¡í„°, ì†ŒìŠ¤ íŒŒì¼ ì •ë³´, ì²­í¬ ID ë“±ì´ í¬í•¨ë©ë‹ˆë‹¤.
        """
        if not chunks or not embeddings:
            print("âš ï¸ ì—…ë¡œë“œí•  ì²­í¬ë‚˜ ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        # ë²Œí¬ ì—…ë¡œë“œ ìµœì í™”
        for i in range(0, len(chunks), self.batch_size):
            actions = []
            chunk_batch = chunks[i:i+self.batch_size]
            embedding_batch = embeddings[i:i+self.batch_size]
            
            # ë²Œí¬ ì•¡ì…˜ êµ¬ì„±
            for idx, (chunk, embedding) in enumerate(zip(chunk_batch, embedding_batch)):
                doc_id = f"{pdf_name}_{i+idx}"
                action = {
                    '_index': self.index_name,
                    '_id': doc_id,
                    '_source': {
                        'content': chunk,
                        'embedding': embedding,
                        'source_pdf': safe_filename,
                        'chunk_id': i+idx,
                        'timestamp': datetime.now().isoformat()
                    }
                }
                actions.append(action)
            
            # ë²Œí¬ ì—…ë¡œë“œ ì‹¤í–‰
            if actions:
                helpers.bulk(self.es, actions, raise_on_error=True)
        
        print(f"â˜ºï¸ {pdf_name}ì˜ {len(chunks)}ê°œ ì²­í¬ë¥¼ ESì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤")